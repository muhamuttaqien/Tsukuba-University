{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25898f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import ai2thor\n",
    "import ai2thor_colab\n",
    "from ai2thor_colab import plot_frames\n",
    "from ai2thor.controller import Controller\n",
    "\n",
    "from ai2thor.platform import CloudRendering\n",
    "controller = Controller(platform=CloudRendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ae6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import DQN\n",
    "from utils import to_torchdim, frame2tensor, plot_durations, encode_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91153cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f0819",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_index = random.randint(0, 30)\n",
    "floor_index = 20\n",
    "\n",
    "controller = Controller(\n",
    "    agentMode = \"default\", # arm\n",
    "    visibilityDistance = 1.5,\n",
    "    scene = f\"FloorPlan{floor_index}\",\n",
    "\n",
    "    # step sizes\n",
    "    snapToGrid = True,\n",
    "    gridSize = 0.25,\n",
    "    rotateStepDegrees = 90,\n",
    "\n",
    "    # image modalities\n",
    "    renderInstanceSegmentation = False,\n",
    "    renderDepthImage = False,\n",
    "    renderSemanticSegmentation = False,\n",
    "    renderNormalsImage = False,\n",
    "    \n",
    "    # camera properties\n",
    "    width = 600,\n",
    "    height = 420,\n",
    "    fieldOfView = 120,\n",
    ")\n",
    "\n",
    "plot_frames(controller.last_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [\"switch\"]\n",
    "# instructions = [\"find light switch\", \"locate power switch\", \"seek light control\", \"discover light control\", \"identify switch light\", \"pinpoint light switch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b15e7",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b65a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44857fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e4)\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "NUM_EPISODES = 3000\n",
    "TARGET_UPDATE = 4\n",
    "\n",
    "TAU = 1e-4\n",
    "LR = 2.5e-4\n",
    "\n",
    "SCREEN_WIDTH = SCREEN_HEIGHT = 100\n",
    "\n",
    "AGENT_TARGET = \"LightSwitch_bf8119ce\" # LightSwitch_887b121a, LightSwitch_bf8119ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [\"MoveAhead\", \"MoveLeft\", \"MoveRight\", \"MoveBack\", \"RotateLeft\", \"RotateRight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac87dfb",
   "metadata": {},
   "source": [
    "## Load Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57457d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = 'weights/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'weights/glove.6B.100d.txt.word2vec'\n",
    "\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412afc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = torch.FloatTensor(word2vec_model.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7cacc0",
   "metadata": {},
   "source": [
    "## Set Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce965a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayMemory object.\"\"\"\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"visual_state\", \"text_state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def add(self, visual_state, text_state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to buffer.\"\"\"\n",
    "        \n",
    "        self.memory.append(self.experience(visual_state, text_state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        visual_states = torch.from_numpy(np.vstack([exp.visual_state.cpu().numpy() for exp in experiences if exp is not None])).float()\n",
    "        visual_states = visual_states.to(device)\n",
    "        \n",
    "        text_states = torch.from_numpy(np.vstack([exp.text_state.cpu().numpy() for exp in experiences if exp is not None])).float()\n",
    "        text_states = text_states.to(device)\n",
    "        \n",
    "        actions = torch.from_numpy(np.vstack([exp.action.cpu().numpy() for exp in experiences if exp is not None])).long()\n",
    "        actions = actions.to(device)\n",
    "        \n",
    "        rewards = torch.from_numpy(np.vstack([exp.reward.cpu().numpy() for exp in experiences if exp is not None])).float()\n",
    "        rewards = rewards.to(device)\n",
    "        \n",
    "        next_states = torch.from_numpy(np.vstack([exp.next_state.cpu().numpy() for exp in experiences if exp is not None])).float()\n",
    "        next_states = next_states.to(device)\n",
    "        \n",
    "        dones = torch.from_numpy(np.vstack([exp.done for exp in experiences if exp is not None]).astype(np.uint8)).float()\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        return (visual_states, text_states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        \n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12125b0",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        state = F.relu(self.bn1(self.conv1(state)))\n",
    "        state = F.relu(self.bn2(self.conv2(state)))\n",
    "        state = F.relu(self.bn3(self.conv3(state)))\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ff9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(VisualModel, self).__init__()\n",
    "        self.cnn = CustomCNN()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.cnn(x)\n",
    "        output = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class TextModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_embedding, hidden_dim):\n",
    "        \n",
    "        super(TextModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=True)\n",
    "        self.rnn = nn.LSTM(pretrained_embedding.shape[1], hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.long()\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = output.view(output.shape[0], -1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb52924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, visual_model, text_model, action_size, seed):\n",
    "        \n",
    "        super(MultimodalDQN, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.visual_model = visual_model\n",
    "        self.text_model = text_model  \n",
    "        self.fc = nn.Linear(2656, action_size)\n",
    "        \n",
    "    def forward(self, visual_input, text_input):\n",
    "        \n",
    "        visual_features = self.visual_model(visual_input.to(device))\n",
    "        text_features = self.text_model(text_input.to(device))\n",
    "        \n",
    "        combined_features = torch.cat((visual_features, text_features), dim=1)\n",
    "        q_values = self.fc(combined_features)\n",
    "        \n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76c67b",
   "metadata": {},
   "source": [
    "## Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    \"\"\"The agent interacting with and learning from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, screen_width, screen_height, action_size, seed):\n",
    "        \"\"\"Init Agent’s models.\"\"\"\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Multimodal DQN\n",
    "        self.visual_model = VisualModel()\n",
    "        self.text_model = TextModel(pretrained_embeddings, hidden_dim=64)\n",
    "\n",
    "        self.dqn_net = MultimodalDQN(self.visual_model, self.text_model, action_size, seed).to(device)\n",
    "        self.target_net = MultimodalDQN(self.visual_model, self.text_model, action_size, seed).to(device)\n",
    "        self.optimizer = optim.RMSprop(self.dqn_net.parameters(), lr=LR, alpha=0.95, eps=0.01)\n",
    "        \n",
    "        # Replay Buffer\n",
    "        self.buffer = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.time_step = 0\n",
    "    \n",
    "    def memorize(self, visual_state, text_state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay buffer.\"\"\"\n",
    "        \n",
    "        self.buffer.add(visual_state, text_state, action, reward, next_state, done)\n",
    "    \n",
    "        self.time_step = (self.time_step + 1) % TARGET_UPDATE\n",
    "        if self.time_step == 0:\n",
    "            # if enough samples are available in memory, get random subset and learn\n",
    "            if len(self.buffer) > BATCH_SIZE:\n",
    "                experiences = self.buffer.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "    \n",
    "    def visual_preprocess(self, visual_state, screen_width, screen_height):\n",
    "        \"\"\"Preprocess input frame before passing into agent.\"\"\"\n",
    "        \n",
    "        resized_screen = Image.fromarray(visual_state).resize((screen_width, screen_height))\n",
    "        visual_state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "\n",
    "        return visual_state\n",
    "    \n",
    "    def text_preprocess(self, instruction):\n",
    "        \"\"\"Preprocess instructions before passing into agent.\"\"\"\n",
    "        \n",
    "        text_state = instruction\n",
    "        text_state = [word2vec_model.key_to_index[word] for word in text_state.split()]\n",
    "        text_state = torch.tensor(text_state).long()\n",
    "        text_state = text_state.unsqueeze(0)\n",
    "        \n",
    "        return text_state\n",
    "    \n",
    "    def act(self, visual_state, text_state, epsilon=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            self.dqn_net.eval()\n",
    "            with torch.no_grad():\n",
    "                action = self.dqn_net(visual_state, text_state).max(1)[1].view(1, 1)\n",
    "            self.dqn_net.train()\n",
    "            \n",
    "        else:\n",
    "            action = torch.tensor([[random.randrange(self.action_size)]], dtype=torch.long, device=device)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\"\"\"\n",
    "    \n",
    "        visual_states, text_states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # get index of maximum value for next state\n",
    "        Qsa_next = self.dqn_net(next_states, text_states).detach()\n",
    "        _, action_max = Qsa_next.max(1)\n",
    "        \n",
    "        # get max predicted Q values (for next states) from target network\n",
    "        Q_target_next = self.target_net(next_states, text_states).detach().gather(1, action_max.unsqueeze(1))\n",
    "        \n",
    "        # compute Q target\n",
    "        Q_target = rewards + (gamma * Q_target_next * (1 - dones))\n",
    "        \n",
    "        # get expected Q values from dqn network\n",
    "        Q_expected = self.dqn_net(visual_states, text_states).gather(1, actions)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(Q_target, Q_expected)\n",
    "        \n",
    "        # minimize the loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # update target network\n",
    "        self.soft_update(self.target_net, self.dqn_net, TAU)\n",
    "    \n",
    "    def soft_update(self, target_net, dqn_net, tau):\n",
    "        \"\"\"Soft update target network parameters.\"\"\"\n",
    "        \n",
    "        for target_param, dqn_param in zip(target_net.parameters(), dqn_net.parameters()):\n",
    "            target_param.data.copy_(tau*dqn_param.data + (1.0-tau) * target_param.data)\n",
    "        \n",
    "    def watch(self, controller, instruction, num_episodes=10):\n",
    "        \"\"\"Watch trained agent.\"\"\"\n",
    "        best_score = -np.inf\n",
    "\n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "            # initialize the environment and state\n",
    "            controller.reset(random=True)\n",
    "\n",
    "            screen = controller.last_event.frame\n",
    "            resized_screen = Image.fromarray(screen).resize((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "\n",
    "            visual_state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "            text_state = agent.text_preprocess(instruction)\n",
    "        \n",
    "            total_score = 0\n",
    "\n",
    "            for time_step in range(1, 100):\n",
    "\n",
    "                # select an action using the trained dqn network\n",
    "                if time_step == 1 or time_step == 2 or time_step == 3:\n",
    "                    action = torch.tensor([[random.randint(0, self.action_size-1)]]).to(device)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        action = self.dqn_net(visual_state, text_state).max(1)[1].view(1, 1)\n",
    "\n",
    "                event = controller.step(action = action_space[action.item()])\n",
    "\n",
    "                time.sleep(1)\n",
    "                \n",
    "                _, reward, done, _ = encode_feedback(event, controller, target_name=AGENT_TARGET)\n",
    "\n",
    "                # observe a new state\n",
    "                if not done:\n",
    "                    screen = controller.last_event.frame\n",
    "                    resized_screen = Image.fromarray(screen).resize((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "\n",
    "                    next_state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "                else:\n",
    "                    next_state = None\n",
    "\n",
    "                visual_state = next_state\n",
    "                total_score += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if total_score > best_score: \n",
    "                best_score = total_score\n",
    "\n",
    "            print(f'\\rEpisode {i_episode}/{num_episodes}, Total Score: {total_score}, Best Score: {best_score}', end='') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(screen_width=SCREEN_WIDTH, screen_height=SCREEN_HEIGHT, action_size=len(action_space), seed=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d1aee2",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define linear decay\n",
    "def calculate_epsilon(episode):\n",
    "    slope = (EPS_END - EPS_START) / NUM_EPISODES\n",
    "    epsilon = EPS_START + slope * episode\n",
    "\n",
    "    return max(epsilon, EPS_END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(num_episodes, max_time):\n",
    "    \n",
    "    epsilon = EPS_START\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        # initialize the environment and state\n",
    "        controller.reset(random=True)\n",
    "        \n",
    "        visual_state = agent.visual_preprocess(controller.last_event.frame, \n",
    "                                               screen_width=SCREEN_WIDTH, screen_height=SCREEN_HEIGHT)\n",
    "        \n",
    "        instruction = instructions[i_episode % len(instructions)]\n",
    "        text_state = agent.text_preprocess(instruction)\n",
    "        \n",
    "        total_score = 0\n",
    "        \n",
    "        for time_step in range(1, max_time+1):\n",
    "            \n",
    "            # select and perform an action using dqn network\n",
    "            action = agent.act(visual_state, text_state, epsilon)\n",
    "            event = controller.step(action = action_space[action.item()])\n",
    "            \n",
    "            _, reward, done, _ = encode_feedback(event, controller, target_name=AGENT_TARGET)\n",
    "            total_score += reward\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            \n",
    "            next_state = agent.visual_preprocess(controller.last_event.frame, \n",
    "                                                 screen_width=SCREEN_WIDTH, screen_height=SCREEN_HEIGHT)\n",
    "            \n",
    "            agent.memorize(visual_state, text_state, action, reward, next_state, done)\n",
    "            \n",
    "            # move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            if done or time_step == max_time:\n",
    "                plot_durations(total_score, i_episode, num_episodes)\n",
    "                break\n",
    "            \n",
    "        epsilon = calculate_epsilon(i_episode)\n",
    "        \n",
    "    if not os.path.exists('./agents/'): os.makedirs('./agents/')\n",
    "    torch.save(agent.dqn_net.state_dict(), f'./agents/AI2THOR_MM_RL.pth')\n",
    "    \n",
    "    print('Training completed.')\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c89f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training the network...')\n",
    "train_network(num_episodes=NUM_EPISODES, max_time=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa572ece",
   "metadata": {},
   "source": [
    "## Check The Result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights of smart agent\n",
    "agent.dqn_net.load_state_dict(torch.load(f'./agents/AI2THOR_MM_RL.pth'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"find light switch\"\n",
    "agent.watch(controller, instruction, num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ebdde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79323260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. try learning with single word\n",
    "# 2. try taking out the hidden layer on text model (only embedding then)\n",
    "# 3. if first & second work, then try three words with embedding layer only\n",
    "# 4. increase num_epsiodes to 4000, target_update to 10 and buffer_size to 2e4\n",
    "# 5. think more about Concatenation technique, Model capacity, Data balance, Loss function and Reward shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfded35",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
