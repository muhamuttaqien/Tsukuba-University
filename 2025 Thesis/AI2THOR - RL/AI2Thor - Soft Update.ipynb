{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523eefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import ai2thor\n",
    "import ai2thor_colab\n",
    "from ai2thor_colab import plot_frames\n",
    "from ai2thor.controller import Controller\n",
    "\n",
    "from ai2thor.platform import CloudRendering\n",
    "controller = Controller(platform=CloudRendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import DQN\n",
    "from utils import to_torchdim, frame2tensor, plot_durations, encode_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446d06a",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_index = random.randint(0, 30)\n",
    "floor_index = 20\n",
    "\n",
    "controller = Controller(\n",
    "    agentMode = \"default\", # arm\n",
    "    visibilityDistance = 1.5,\n",
    "    scene = f\"FloorPlan{floor_index}\",\n",
    "\n",
    "    # step sizes\n",
    "    snapToGrid = True,\n",
    "    gridSize = 0.25,\n",
    "    rotateStepDegrees = 90,\n",
    "\n",
    "    # image modalities\n",
    "    renderInstanceSegmentation = False,\n",
    "    renderDepthImage = False,\n",
    "    renderSemanticSegmentation = False,\n",
    "    renderNormalsImage = False,\n",
    "    \n",
    "    # camera properties\n",
    "    width = 600,\n",
    "    height = 420,\n",
    "    fieldOfView = 120,\n",
    ")\n",
    "\n",
    "plot_frames(controller.last_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a9796",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e4) # experience buffer/memory\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.999 # discounted return\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "NUM_EPISODES = 3000\n",
    "TARGET_UPDATE = 4 # hard update\n",
    "\n",
    "TAU = 1e-4 # for soft update of target parameters\n",
    "LR = 2.5e-4\n",
    "\n",
    "SCREEN_WIDTH = SCREEN_HEIGHT = 100\n",
    "\n",
    "AGENT_TARGET = \"LightSwitch_bf8119ce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65219f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_space = [\"MoveAhead\", \"MoveLeft\", \"MoveRight\", \"MoveBack\", \"RotateLeft\", \"RotateRight\", \"OpenObject\", \"ToggleObjectOn\"]\n",
    "action_space = [\"MoveAhead\", \"MoveLeft\", \"MoveRight\", \"MoveBack\", \"RotateLeft\", \"RotateRight\"]\n",
    "# action_space = [\"MoveAhead\", \"MoveBack\", \"RotateLeft\", \"RotateRight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7696f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = controller.last_event.metadata['objects']\n",
    "object_space = [item['name'] for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6056fd37",
   "metadata": {},
   "source": [
    "## Set Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbe63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayMemory object.\"\"\"\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to buffer.\"\"\"\n",
    "        \n",
    "        self.memory.append(self.experience(state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([exp.state.cpu().numpy() for exp in experiences if exp is not None])).float()\n",
    "        states = states.to(device)\n",
    "        \n",
    "        actions = torch.from_numpy(np.vstack([exp.action.cpu().numpy() for exp in experiences if exp is not None])).long()\n",
    "        actions = actions.to(device)\n",
    "        \n",
    "        rewards = torch.from_numpy(np.vstack([exp.reward.cpu().numpy() for exp in experiences if exp is not None])).float()\n",
    "        rewards = rewards.to(device)\n",
    "        \n",
    "        next_states = torch.from_numpy(np.vstack([exp.next_state.cpu().numpy() for exp in experiences if exp is not None])).float()\n",
    "        next_states = next_states.to(device)\n",
    "        \n",
    "        dones = torch.from_numpy(np.vstack([exp.done for exp in experiences if exp is not None]).astype(np.uint8)).float()\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33cab06",
   "metadata": {},
   "source": [
    "## Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fba230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    \"\"\"The agent interacting with and learning from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, screen_width, screen_height, action_size, seed):\n",
    "        \"\"\"Init Agentâ€™s models.\"\"\"\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Q-Network\n",
    "        self.dqn_net = DQN(screen_width, screen_height, action_size, seed).to(device)\n",
    "        self.target_net = DQN(screen_width, screen_height, action_size, seed).to(device)\n",
    "        self.optimizer = optim.RMSprop(self.dqn_net.parameters(), lr=LR, alpha=0.95, eps=0.01)\n",
    "        \n",
    "        # Replay Buffer\n",
    "        self.buffer = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.time_step = 0\n",
    "        \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay buffer.\"\"\"\n",
    "        \n",
    "        self.buffer.add(state, action, reward, next_state, done)\n",
    "    \n",
    "        self.time_step = (self.time_step + 1) % TARGET_UPDATE\n",
    "        if self.time_step == 0:\n",
    "            # if enough samples are available in memory, get random subset and learn\n",
    "            if len(self.buffer) > BATCH_SIZE:\n",
    "                experiences = self.buffer.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "                \n",
    "    def preprocess(self, state, screen_width, screen_height):\n",
    "        \"\"\"Preprocess input frame before passing into agent.\"\"\"\n",
    "        \n",
    "        resized_screen = Image.fromarray(state).resize((screen_width, screen_height))\n",
    "        state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def act(self, state, epsilon=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            self.dqn_net.eval()\n",
    "            with torch.no_grad():\n",
    "                action = self.dqn_net(state).max(1)[1].view(1, 1)\n",
    "            self.dqn_net.train()\n",
    "            \n",
    "        else:\n",
    "            action = torch.tensor([[random.randrange(self.action_size)]], dtype=torch.long, device=device)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\"\"\"\n",
    "    \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # get index of maximum value for next state\n",
    "        Qsa_next = self.dqn_net(next_states).detach()\n",
    "        _, action_max = Qsa_next.max(1)\n",
    "\n",
    "        # get max predicted Q values (for next states) from target network\n",
    "        Q_target_next = self.target_net(next_states).detach().gather(1, action_max.unsqueeze(1))\n",
    "        \n",
    "        # compute Q target\n",
    "        Q_target = rewards + (gamma * Q_target_next * (1 - dones))\n",
    "        \n",
    "        # get expected Q values from dqn network\n",
    "        Q_expected = self.dqn_net(states).gather(1, actions)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(Q_target, Q_expected)\n",
    "        \n",
    "        # minimize the loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # update target network\n",
    "        self.soft_update(self.target_net, self.dqn_net, TAU)\n",
    "        \n",
    "    def soft_update(self, target_net, dqn_net, tau):\n",
    "        \"\"\"Soft update target network parameters.\"\"\"\n",
    "        \n",
    "        for target_param, dqn_param in zip(target_net.parameters(), dqn_net.parameters()):\n",
    "            target_param.data.copy_(tau*dqn_param.data + (1.0-tau) * target_param.data)\n",
    "            \n",
    "    def watch(self, controller, num_episodes=10):\n",
    "        \"\"\"Watch trained agent.\"\"\"\n",
    "        best_score = -np.inf\n",
    "\n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "            # initialize the environment and state\n",
    "            controller.reset(random=True)\n",
    "\n",
    "            screen = controller.last_event.frame\n",
    "            resized_screen = Image.fromarray(screen).resize((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "\n",
    "            state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "\n",
    "            total_score = 0\n",
    "\n",
    "            for time_step in range(1, 100):\n",
    "\n",
    "                # select an action using the trained dqn network\n",
    "                if time_step == 1 or time_step == 2 or time_step == 3:\n",
    "                    action = torch.tensor([[random.randint(0, self.action_size-1)]]).to(device)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        action = self.dqn_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "                event = controller.step(action = action_space[action.item()])\n",
    "\n",
    "                time.sleep(1)\n",
    "                \n",
    "                _, reward, done, _ = encode_feedback(event, controller, target_name=AGENT_TARGET)\n",
    "\n",
    "                # observe a new state\n",
    "                if not done:\n",
    "                    screen = controller.last_event.frame\n",
    "                    resized_screen = Image.fromarray(screen).resize((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "\n",
    "                    next_state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "                else:\n",
    "                    next_state = None\n",
    "\n",
    "                state = next_state\n",
    "                total_score += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if total_score > best_score: \n",
    "                best_score = total_score\n",
    "\n",
    "            print(f'\\rEpisode {i_episode}/{num_episodes}, Total Score: {total_score}, Best Score: {best_score}', end='') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49260cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(screen_width=SCREEN_WIDTH, screen_height=SCREEN_HEIGHT, action_size=len(action_space), seed=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b93166",
   "metadata": {},
   "source": [
    "## Train DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af897a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define linear decay\n",
    "def calculate_epsilon(episode):\n",
    "    slope = (EPS_END - EPS_START) / NUM_EPISODES\n",
    "    epsilon = EPS_START + slope * episode\n",
    "\n",
    "    return max(epsilon, EPS_END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(num_episodes, max_time):\n",
    "    \n",
    "    epsilon = EPS_START\n",
    "\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        # initialize the environment and state\n",
    "        controller.reset(random=True)\n",
    "        \n",
    "        state = agent.preprocess(controller.last_event.frame, \n",
    "                                 screen_width=SCREEN_WIDTH, screen_height=SCREEN_HEIGHT)\n",
    "                                  \n",
    "        total_score = 0\n",
    "        \n",
    "        for time_step in range(1, max_time+1):\n",
    "            \n",
    "            # select and perform an action using dqn network\n",
    "            action = agent.act(state, epsilon)\n",
    "            event = controller.step(action = action_space[action.item()])\n",
    "            \n",
    "            _, reward, done, _ = encode_feedback(event, controller, target_name=AGENT_TARGET)\n",
    "            total_score += reward\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            next_state = agent.preprocess(controller.last_event.frame, \n",
    "                                          screen_width=SCREEN_WIDTH, screen_height=SCREEN_HEIGHT)\n",
    "            \n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "\n",
    "            # move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done or time_step==max_time:\n",
    "                plot_durations(total_score, i_episode, num_episodes)\n",
    "                break\n",
    "\n",
    "        epsilon = calculate_epsilon(i_episode)\n",
    "\n",
    "    if not os.path.exists('./agents/'): os.makedirs('./agents/')\n",
    "    torch.save(agent.dqn_net.state_dict(), f'./agents/AI2THOR_RL.pth')\n",
    "                \n",
    "    print('Training completed.')\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training the network...')\n",
    "train_network(num_episodes=NUM_EPISODES, max_time=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c490f27",
   "metadata": {},
   "source": [
    "## Check The Result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights of smart agent\n",
    "agent.dqn_net.load_state_dict(torch.load(f'./agents/AI2THOR_RL.pth'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2b757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.watch(controller, num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfded35",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
