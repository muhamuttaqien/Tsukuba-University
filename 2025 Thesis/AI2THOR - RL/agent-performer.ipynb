{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import ai2thor\n",
    "import ai2thor_colab\n",
    "from ai2thor_colab import plot_frames\n",
    "from ai2thor.controller import Controller\n",
    "\n",
    "from ai2thor.platform import CloudRendering\n",
    "controller = Controller(platform=CloudRendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cfc8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import DQN\n",
    "from utils import to_torchdim, frame2tensor, encode_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2507d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965aca0",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_index = random.randint(0, 30)\n",
    "floor_index = 20\n",
    "\n",
    "controller = Controller(\n",
    "    agentMode = \"default\", # arm\n",
    "    visibilityDistance = 0.75,\n",
    "    scene = f\"FloorPlan{floor_index}\",\n",
    "\n",
    "    # step sizes\n",
    "    snapToGrid = True,\n",
    "    gridSize = 0.25,\n",
    "    rotateStepDegrees = 90,\n",
    "\n",
    "    # image modalities\n",
    "    renderInstanceSegmentation = False,\n",
    "    renderDepthImage = False,\n",
    "    renderSemanticSegmentation = False,\n",
    "    renderNormalsImage = False,\n",
    "    \n",
    "    # camera properties\n",
    "    width = 600,\n",
    "    height = 420,\n",
    "    fieldOfView = 120,\n",
    "    \n",
    "    # set seed for reproducability\n",
    "    seed = 90,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46a567",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 10000 # 6000\n",
    "\n",
    "SCREEN_WIDTH = SCREEN_HEIGHT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9612bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [\"MoveAhead\", \"MoveLeft\", \"MoveRight\", \"MoveBack\", \"RotateLeft\", \"RotateRight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dedfbac",
   "metadata": {},
   "source": [
    "## Load Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = 'weights/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'weights/glove.6B.100d.txt.word2vec'\n",
    "\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = torch.FloatTensor(word2vec_model.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe5c6fc",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af3df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        \n",
    "        super(VisualModel, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.cnn(x)\n",
    "    \n",
    "    \n",
    "class TextModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_embedding, hidden_dim, seed):\n",
    "        \n",
    "        super(TextModel, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=True)\n",
    "        self.rnn = nn.LSTM(pretrained_embedding.shape[1], hidden_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.long()\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = output.view(output.shape[0], -1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, visual_model, text_model, action_size, seed):\n",
    "        \n",
    "        super(MultimodalDQN, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.visual_model = visual_model\n",
    "        self.text_model = text_model\n",
    "        \n",
    "        # Define three fully connected layers\n",
    "        self.fc1 = nn.Linear(576, 512) # 256, 512, 1024\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, action_size)\n",
    "        \n",
    "    def forward(self, visual_input, text_input):\n",
    "        \n",
    "        visual_features = self.visual_model(visual_input.to(device))\n",
    "        text_features = self.text_model(text_input.to(device))\n",
    "        \n",
    "        # Concatenate visual and text features\n",
    "        combined_features = torch.cat((visual_features, text_features), dim=1)\n",
    "        \n",
    "        # Apply fully connected layers\n",
    "        combined_features = F.relu(self.fc1(combined_features))\n",
    "        combined_features = F.relu(self.fc2(combined_features))\n",
    "        q_values = self.fc3(combined_features)\n",
    "        \n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc0d08",
   "metadata": {},
   "source": [
    "## Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7266a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    \"\"\"The agent interacting with and learning from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, screen_width, screen_height, action_size, seed):\n",
    "        \"\"\"Init Agentâ€™s models.\"\"\"\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Multimodal DQN\n",
    "        self.visual_model = VisualModel(seed=seed)\n",
    "        self.text_model = TextModel(pretrained_embeddings, hidden_dim=64, seed=seed)\n",
    "\n",
    "        self.dqn_net = MultimodalDQN(self.visual_model, self.text_model, action_size, seed).to(device)\n",
    "    \n",
    "    def visual_preprocess(self, visual_state, screen_width, screen_height):\n",
    "        \"\"\"Preprocess input frame before passing into agent.\"\"\"\n",
    "        \n",
    "        resized_screen = Image.fromarray(visual_state).resize((screen_width, screen_height))\n",
    "        visual_state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "\n",
    "        return visual_state\n",
    "    \n",
    "    def text_preprocess(self, instruction):\n",
    "        \"\"\"Preprocess instructions before passing into agent.\"\"\"\n",
    "        \n",
    "        text_state = instruction\n",
    "        text_state = [word2vec_model.key_to_index[word] for word in text_state.split()]\n",
    "        text_state = torch.tensor(text_state).long()\n",
    "        text_state = text_state.unsqueeze(0)\n",
    "        \n",
    "        return text_state\n",
    "    \n",
    "    def randomize_agent(self, controller):\n",
    "\n",
    "        positions = controller.step(\n",
    "            action=\"GetReachablePositions\"\n",
    "        ).metadata[\"actionReturn\"]\n",
    "\n",
    "        position = random.choice(positions)\n",
    "        controller.step(\n",
    "            action=\"Teleport\",\n",
    "            position=position,\n",
    "            rotation=dict(x=0, y=270, z=0),\n",
    "            horizon=0,\n",
    "            standing=True\n",
    "        )\n",
    "        \n",
    "    def watch(self, controller, instruction, num_episodes=10):\n",
    "        \"\"\"Watch trained agent.\"\"\"\n",
    "        \n",
    "        best_score = -np.inf\n",
    "        action_space = [\"MoveAhead\", \"MoveLeft\", \"MoveRight\", \"MoveBack\", \"RotateLeft\", \"RotateRight\"]\n",
    "\n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "            # initialize the environment and state\n",
    "            controller.reset(random=True)\n",
    "            \n",
    "            self.randomize_agent(controller)\n",
    "\n",
    "            visual_state = agent.visual_preprocess(controller.last_event.frame, \n",
    "                                               screen_width=SCREEN_WIDTH, screen_height=SCREEN_HEIGHT)\n",
    "        \n",
    "            inst_tupple = instructions[i_episode % len(instructions)]\n",
    "            instruction, AGENT_TARGET = inst_tupple\n",
    "            text_state = agent.text_preprocess(instruction)\n",
    "            \n",
    "            total_score = 0\n",
    "\n",
    "            self.dqn_net.eval()\n",
    "                \n",
    "            for time_step in range(1, 50):\n",
    "                \n",
    "                # clear_output(wait=True)\n",
    "                \n",
    "                # select an action using the trained dqn network\n",
    "                with torch.no_grad():\n",
    "                    action = self.dqn_net(visual_state, text_state).max(1)[1].view(1, 1)\n",
    "\n",
    "                # print(f\"Time Step: {time_step}, Action: {action_space[action.item()]}\")\n",
    "                event = controller.step(action = action_space[action.item()])\n",
    "\n",
    "                time.sleep(1)\n",
    "                \n",
    "                _, reward, done, _ = encode_feedback(event, controller, target_name=AGENT_TARGET)\n",
    "\n",
    "                # observe a new state\n",
    "                if not done:\n",
    "                    screen = controller.last_event.frame\n",
    "                    resized_screen = Image.fromarray(screen).resize((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "\n",
    "                    next_state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "                else:\n",
    "                    next_state = None\n",
    "\n",
    "                visual_state = next_state\n",
    "                total_score += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if total_score > best_score: \n",
    "                best_score = total_score\n",
    "\n",
    "            print(f'\\rEpisode {i_episode}/{num_episodes}, Total Step: {time_step}, Total Score: {total_score}, Best Score: {best_score}', end='') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd71f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(screen_width=SCREEN_WIDTH, screen_height=SCREEN_HEIGHT, action_size=len(action_space), seed=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d324989",
   "metadata": {},
   "source": [
    "## Check The Result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights of smart agent\n",
    "agent.dqn_net.load_state_dict(torch.load(f'./agents/AI2THOR_MM_RL_3OBJ_R20.pth'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [(\"switch\", \"LightSwitch_887b121a\")]\n",
    "# instructions = [(\"tomato\", \"Tomato_e65a6e2e\")]\n",
    "# instructions = [(\"garbage\", \"GarbageCan_d6916cf5\")]\n",
    "\n",
    "agent.watch(controller, instructions, num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e79dbd",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
