{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523eefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import ai2thor\n",
    "import ai2thor_colab\n",
    "from ai2thor_colab import plot_frames\n",
    "from ai2thor.controller import Controller\n",
    "\n",
    "from ai2thor.platform import CloudRendering\n",
    "controller = Controller(platform=CloudRendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import DQN\n",
    "from utils import to_torchdim, frame2tensor, plot_durations, encode_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a9796",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = int(1e4)\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 20e3\n",
    "TARGET_UPDATE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eebd6b1",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed9a9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "floor_index = random.randint(0, 30)\n",
    "floor_index = 20\n",
    "\n",
    "controller = Controller(\n",
    "    agentMode = \"default\", # arm\n",
    "    visibilityDistance = 1.5,\n",
    "    scene = f\"FloorPlan{floor_index}\",\n",
    "\n",
    "    # step sizes\n",
    "    gridSize = 0.25,\n",
    "    snapToGrid = True,\n",
    "    rotateStepDegrees = 90,\n",
    "\n",
    "    # image modalities\n",
    "    renderInstanceSegmentation = False,\n",
    "    renderDepthImage = False,\n",
    "    renderSemanticSegmentation = False,\n",
    "    renderNormalsImage = False,\n",
    "    \n",
    "    # camera properties\n",
    "    width = 600,\n",
    "    height = 420,\n",
    "    fieldOfView = 120,\n",
    ")\n",
    "\n",
    "plot_frames(controller.last_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca02426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_space = [\"MoveAhead\", \"MoveLeft\", \"MoveRight\", \"MoveBack\", \"RotateLeft\", \"RotateRight\"]\n",
    "action_space = [\"MoveAhead\", \"MoveBack\", \"RotateLeft\", \"RotateRight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007afce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = controller.last_event.metadata['objects']\n",
    "object_space = [item['name'] for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6056fd37",
   "metadata": {},
   "source": [
    "## Set Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305686e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', \n",
    "                       ('state', 'action', 'reward', 'next_state'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbe63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method for selecting a random batch of transitions for training\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"Fixed-size memory to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    # *args filled by state, action, next_state and reward variables\n",
    "    def push(self, *args):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        \n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            \n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(MEMORY_SIZE) # init capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676a9b0",
   "metadata": {},
   "source": [
    "## Initialize DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ac610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the height, width of the observation (image) and action size from the environment\n",
    "# screen_width = controller.last_event.metadata[\"screenWidth\"]\n",
    "# screen_height = controller.last_event.metadata[\"screenHeight\"]\n",
    "screen_width = 100\n",
    "screen_height = 100\n",
    "\n",
    "n_actions = len(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7dffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_net = DQN(screen_width, screen_height, n_actions).to(device)\n",
    "target_net = DQN(screen_width, screen_height, n_actions).to(device)\n",
    "target_net.load_state_dict(dqn_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0226e4",
   "metadata": {},
   "source": [
    "## Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49260cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(dqn_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b93166",
   "metadata": {},
   "source": [
    "## Train DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af897a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "# this will select an action accordingly to an epsilon greedy policy\n",
    "def select_action(state):\n",
    "    \n",
    "    global steps_done\n",
    "    \n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # here the network will pick action with the larger expected reward\n",
    "            action = dqn_net(state).max(1)[1].view(1, 1)\n",
    "            return action\n",
    "    else:\n",
    "        action = torch.tensor([[random.randrange(n_actions)]], dtype=torch.long, device=device)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315638da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function first samples a batch, concatenates all the tensors into a single one\n",
    "# then computes Q(st, at) and V(st+1) = maxaQ(st+1, a), and combines them into our loss\n",
    "def optimize_network():\n",
    "    \n",
    "    # this will skip the optimization process if there is no enough memory\n",
    "    if len(memory) < BATCH_SIZE: return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # this converts batch-array of transitions to transition of batch-arrays\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), dtype=torch.uint8, device=device)\n",
    "    non_final_next_states = torch.cat([state for state in batch.next_state if state is not None])\n",
    "    \n",
    "    # concatenate all states, actions and rewards on the batch\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # compute V(s_{t+1}) for all next states\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # compute Q target\n",
    "    Q_target = reward_batch + (GAMMA * next_state_values)\n",
    "    \n",
    "    # get the expected Q values\n",
    "    # the network computes Q(s_t) then will select the columns of actions (a) taken\n",
    "    # The (a) is the actions which would've been taken for each batch state according to dqn_net\n",
    "    Q_expected = dqn_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # compute huber loss\n",
    "    loss = F.smooth_l1_loss(Q_expected, Q_target.unsqueeze(1))\n",
    "    \n",
    "    # this will perform optimization for the network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in dqn_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = controller.last_event.frame\n",
    "resized_screen = Image.fromarray(screen).resize((150, 150))\n",
    "\n",
    "state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(num_episodes, max_time):\n",
    "\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        # initialize the environment and state\n",
    "        controller.reset(random=True)\n",
    "\n",
    "        screen = controller.last_event.frame\n",
    "        resized_screen = Image.fromarray(screen).resize((screen_width, screen_height))\n",
    "\n",
    "        state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "                                  \n",
    "        total_score = 0\n",
    "        \n",
    "        for time_step in range(1, max_time+1):\n",
    "            \n",
    "            # select and perform an action using dqn network\n",
    "            action = select_action(state)\n",
    "            event = controller.step(action = action_space[action.item()])\n",
    "            \n",
    "            _, reward, done, _ = encode_feedback(event, controller, target_name=\"LightSwitch_887b121a\")\n",
    "            total_score += reward\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if not done:\n",
    "                screen = controller.last_event.frame\n",
    "                resized_screen = Image.fromarray(screen).resize((screen_width, screen_height))\n",
    "                \n",
    "                next_state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # store the transition in memory\n",
    "            memory.push(state, action, reward, next_state)\n",
    "\n",
    "            # move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # perform one step of the optimization on the target network\n",
    "            optimize_network()\n",
    "\n",
    "            if done or time_step==max_time:\n",
    "                plot_durations(total_score, i_episode, num_episodes)\n",
    "                break\n",
    "\n",
    "            # update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(dqn_net.state_dict())\n",
    "\n",
    "    if not os.path.exists('./agents/'): os.makedirs('./agents/')\n",
    "    torch.save(dqn_net.state_dict(), f'./agents/AI2THOR_RL.pth')\n",
    "    print('Training completed.')\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d829a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training the network...')\n",
    "train_network(num_episodes=1000, max_time=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c490f27",
   "metadata": {},
   "source": [
    "## Check The Result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights of smart agent\n",
    "dqn_net.load_state_dict(torch.load(f'./agents/AI2THOR_RL.pth'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 20\n",
    "best_score = -np.inf\n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    \n",
    "    # initialize the environment and state\n",
    "    controller.reset(random=True)\n",
    "\n",
    "    screen = controller.last_event.frame\n",
    "    resized_screen = Image.fromarray(screen).resize((screen_width, screen_height))\n",
    "\n",
    "    state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "    \n",
    "    total_score = 0\n",
    "    \n",
    "    for time_step in range(1, 100):\n",
    "        \n",
    "        # select an action using the trained dqn network\n",
    "        with torch.no_grad():\n",
    "            action = dqn_net(state).max(1)[1].view(1, 1)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"Episode:\", i_episode+1)\n",
    "        print(\"Action:\", time_step+1)\n",
    "        if action_space[action.item()] == \"RotateLeft\" or action_space[action.item()] == \"RotateRight\":\n",
    "            print(\"Action:\\033[91m\", action_space[action.item()])\n",
    "        else:\n",
    "            print(\"Action:\\033[92m\", action_space[action.item()])\n",
    "\n",
    "        event = controller.step(action = action_space[action.item()])\n",
    "        \n",
    "        time.sleep(1)\n",
    "            \n",
    "        _, reward, done, _ = encode_feedback(event, controller, target_name=\"LightSwitch_887b121a\")\n",
    "            \n",
    "        # observe a new state\n",
    "        if not done:\n",
    "            screen = controller.last_event.frame\n",
    "            resized_screen = Image.fromarray(screen).resize((screen_width, screen_height))\n",
    "\n",
    "            next_state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "        else:\n",
    "            next_state = None\n",
    "                \n",
    "        state = next_state\n",
    "        total_score += reward\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    if total_score > best_score: \n",
    "        best_score = total_score\n",
    "        \n",
    "    print(f'\\rEpisode {i_episode}/{num_episodes}, Best Score: {best_score}', end='')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3525c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo list\n",
    "# 1. Modify DQN to align with AI2THOR not Gym ✅\n",
    "# 2. Modify train_network to align with AI2THOR not Gym ✅\n",
    "# 3. Design reward ✅\n",
    "# 4. Change epsilon strategy ✅\n",
    "# 5. Apply Random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05028803",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
