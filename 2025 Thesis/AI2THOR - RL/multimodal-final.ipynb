{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c68a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f071ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e288e29d",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363ff52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(VisualModel, self).__init__()\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.cnn(x)\n",
    "    \n",
    "    \n",
    "class TextModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_embedding, hidden_dim):\n",
    "        \n",
    "        super(TextModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=True)\n",
    "        self.rnn = nn.LSTM(pretrained_embedding.shape[1], hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        return output[:, :, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35243ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, visual_model, text_model, action_space):\n",
    "        \n",
    "        super(MultimodalDQN, self).__init__()\n",
    "        self.visual_model = visual_model\n",
    "        self.text_model = text_model\n",
    "        self.fc1 = nn.Linear(515, 4120)  \n",
    "        self.fc2 = nn.Linear(4120, action_space)\n",
    "        \n",
    "    def forward(self, visual_input, text_input):\n",
    "        \n",
    "        visual_features = self.visual_model(visual_input)\n",
    "        text_features = self.text_model(text_input)\n",
    "        \n",
    "        combined_features = torch.cat((visual_features, text_features), dim=1)\n",
    "        combined_features = self.fc1(combined_features)\n",
    "        q_values = self.fc2(combined_features)\n",
    "        \n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60efa4f",
   "metadata": {},
   "source": [
    "## Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941f5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory.pop(0)\n",
    "            self.memory.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1934b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_update(model, target_model, optimizer, batch, gamma):\n",
    "    \n",
    "    visual_state, text_state, action, reward, next_state = zip(*batch)\n",
    "\n",
    "    visual_state = torch.stack(visual_state)\n",
    "    text_state = torch.stack(text_state)\n",
    "    action = torch.tensor(action)\n",
    "    reward = torch.tensor(reward)\n",
    "    next_state = torch.stack(next_state)\n",
    "\n",
    "    q_values = model(visual_state, text_state)\n",
    "    next_q_values = target_model(visual_state, text_state).max(1).values.detach()\n",
    "    expected_q_values = reward + gamma * next_q_values\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss_value = loss(q_values.gather(1, action.unsqueeze(1)), expected_q_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_value.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f303c1",
   "metadata": {},
   "source": [
    "## Load Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a823fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff9fcbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107258/3840209020.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "glove_input_file = 'weights/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'weights/glove.6B.100d.txt.word2vec'\n",
    "\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a5113cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = torch.FloatTensor(word2vec_model.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f886a58",
   "metadata": {},
   "source": [
    "## Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a090ec89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muhamuttaqien/miniconda3/envs/thesis/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/muhamuttaqien/miniconda3/envs/thesis/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "visual_model = VisualModel()\n",
    "text_model = TextModel(pretrained_embeddings, hidden_dim=64)\n",
    "\n",
    "model = MultimodalDQN(visual_model, text_model, action_space=4)\n",
    "target_model = MultimodalDQN(visual_model, text_model, action_space=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95dc3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "replay_buffer = ReplayBuffer(capacity=1000)\n",
    "gamma = 0.9 # Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c88d3",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c82cf468",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\"light\": 0, \"switch\": 1, \"go\": 2, \"find\": 3}\n",
    "\n",
    "images = [torch.randn(3, 64, 64) for _ in range(10)]\n",
    "instructions = [\"find light switch\" for _ in range(10)]\n",
    "actions = [0, 1, 1, 0, 0, 1, 0, 1, 0, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7751486c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch: 1\n",
      "--- Epoch: 2\n",
      "--- Epoch: 3\n",
      "--- Epoch: 4\n",
      "--- Epoch: 5\n",
      "--- Epoch: 6\n",
      "--- Epoch: 7\n",
      "--- Epoch: 8\n",
      "--- Epoch: 9\n",
      "--- Epoch: 10\n",
      "--- Epoch: 11\n",
      "--- Epoch: 12\n",
      "--- Epoch: 13\n",
      "--- Epoch: 14\n",
      "--- Epoch: 15\n",
      "--- Epoch: 16\n",
      "--- Epoch: 17\n",
      "--- Epoch: 18\n",
      "--- Epoch: 19\n",
      "--- Epoch: 20\n",
      "--- Epoch: 21\n",
      "--- Epoch: 22\n",
      "--- Epoch: 23\n",
      "--- Epoch: 24\n",
      "--- Epoch: 25\n",
      "--- Epoch: 26\n",
      "--- Epoch: 27\n",
      "--- Epoch: 28\n",
      "--- Epoch: 29\n",
      "--- Epoch: 30\n",
      "--- Epoch: 31\n",
      "--- Epoch: 32\n",
      "--- Epoch: 33\n",
      "--- Epoch: 34\n",
      "--- Epoch: 35\n",
      "--- Epoch: 36\n",
      "--- Epoch: 37\n",
      "--- Epoch: 38\n",
      "--- Epoch: 39\n",
      "--- Epoch: 40\n",
      "--- Epoch: 41\n",
      "--- Epoch: 42\n",
      "--- Epoch: 43\n",
      "--- Epoch: 44\n",
      "--- Epoch: 45\n",
      "--- Epoch: 46\n",
      "--- Epoch: 47\n",
      "--- Epoch: 48\n",
      "--- Epoch: 49\n",
      "--- Epoch: 50\n",
      "--- Epoch: 51\n",
      "--- Epoch: 52\n",
      "--- Epoch: 53\n",
      "--- Epoch: 54\n",
      "--- Epoch: 55\n",
      "--- Epoch: 56\n",
      "--- Epoch: 57\n",
      "--- Epoch: 58\n",
      "--- Epoch: 59\n",
      "--- Epoch: 60\n",
      "--- Epoch: 61\n",
      "--- Epoch: 62\n",
      "--- Epoch: 63\n",
      "--- Epoch: 64\n",
      "--- Epoch: 65\n",
      "--- Epoch: 66\n",
      "--- Epoch: 67\n",
      "--- Epoch: 68\n",
      "--- Epoch: 69\n",
      "--- Epoch: 70\n",
      "--- Epoch: 71\n",
      "--- Epoch: 72\n",
      "--- Epoch: 73\n",
      "--- Epoch: 74\n",
      "--- Epoch: 75\n",
      "--- Epoch: 76\n",
      "--- Epoch: 77\n",
      "--- Epoch: 78\n",
      "--- Epoch: 79\n",
      "--- Epoch: 80\n",
      "--- Epoch: 81\n",
      "--- Epoch: 82\n",
      "--- Epoch: 83\n",
      "--- Epoch: 84\n",
      "--- Epoch: 85\n",
      "--- Epoch: 86\n",
      "--- Epoch: 87\n",
      "--- Epoch: 88\n",
      "--- Epoch: 89\n",
      "--- Epoch: 90\n",
      "--- Epoch: 91\n",
      "--- Epoch: 92\n",
      "--- Epoch: 93\n",
      "--- Epoch: 94\n",
      "--- Epoch: 95\n",
      "--- Epoch: 96\n",
      "--- Epoch: 97\n",
      "--- Epoch: 98\n",
      "--- Epoch: 99\n",
      "--- Epoch: 100\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    \n",
    "    print(\"--- Epoch:\", epoch+1)\n",
    "    visual_state = images[epoch % len(images)]\n",
    "    text_state = instructions[epoch % len(instructions)]\n",
    "    text_state = [vocab[word] for word in text_state.split()]\n",
    "    text_state = torch.LongTensor(text_state)\n",
    "    action = actions[epoch % len(actions)]\n",
    "    next_state = images[(epoch + 1) % len(images)]\n",
    "    \n",
    "    replay_buffer.push((visual_state, text_state, action, 1.0, next_state))\n",
    "    \n",
    "    if len(replay_buffer.memory) > 32:\n",
    "        batch = replay_buffer.sample(32)\n",
    "        q_learning_update(model, target_model, optimizer, batch, gamma)\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        target_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c1228",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1e1973d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted action: 0\n"
     ]
    }
   ],
   "source": [
    "visual_input = images[0].unsqueeze(0)\n",
    "text_input = instructions[0]\n",
    "text_input = [vocab[word] for word in text_input.split()]\n",
    "text_input = torch.LongTensor(text_input).unsqueeze(0)\n",
    "    \n",
    "q_values = model(visual_input, text_input)\n",
    "action = q_values.argmax().item()\n",
    "\n",
    "print(f\"Predicted action: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05028803",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
