{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef3949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muttaqien-m/anaconda3/envs/thesis/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/muttaqien-m/anaconda3/envs/thesis/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2935bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba30d2e",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69461565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(VisualModel, self).__init__()\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        # self.cnn.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.cnn(x)\n",
    "    \n",
    "    \n",
    "class TextModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_embedding, hidden_dim):\n",
    "        \n",
    "        super(TextModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=True)\n",
    "        self.rnn = nn.LSTM(pretrained_embedding.shape[1], hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        return output[..., -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "534cfb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, visual_model, text_model, action_space):\n",
    "        \n",
    "        super(MultimodalDQN, self).__init__()\n",
    "        self.visual_model = visual_model\n",
    "        self.text_model = text_model\n",
    "        self.fc1 = nn.Linear(515, 4120)  \n",
    "        self.fc2 = nn.Linear(4120, action_space)\n",
    "        \n",
    "    def forward(self, visual_input, text_input):\n",
    "        \n",
    "        visual_features = self.visual_model(visual_input) # shape 32x512\n",
    "        text_features = self.text_model(text_input) # shape 32x3\n",
    "                \n",
    "        print(\"--- visual_features:\", visual_features.shape)\n",
    "        print(\"--- text_features:\", text_features.shape)\n",
    "        \n",
    "        combined_features = torch.cat((visual_features, text_features), dim=1) # shape 32x515\n",
    "        combined_features = self.fc1(combined_features) # shape 32x4120\n",
    "        q_values = self.fc2(combined_features) # shape 32x4\n",
    "        \n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a55e58",
   "metadata": {},
   "source": [
    "## Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38aff121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory.pop(0)\n",
    "            self.memory.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "579ff71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_update(model, target_model, optimizer, batch, gamma):\n",
    "    \n",
    "    visual_state, text_state, action, reward, next_state = zip(*batch)\n",
    "\n",
    "    visual_state = torch.stack(visual_state)\n",
    "    text_state = torch.stack(text_state)\n",
    "    action = torch.tensor(action)\n",
    "    reward = torch.tensor(reward)\n",
    "    next_state = torch.stack(next_state)\n",
    "    \n",
    "    q_values = model(visual_state, text_state)\n",
    "    next_q_values = target_model(visual_state, text_state).max(1).values.detach()\n",
    "    expected_q_values = reward + gamma * next_q_values\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss_value = loss(expected_q_values.unsqueeze(1), q_values.gather(1, action.unsqueeze(1)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_value.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867225c5",
   "metadata": {},
   "source": [
    "## Load Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e68d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "270f8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = 'weights/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'weights/glove.6B.100d.txt.word2vec'\n",
    "\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf3c472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = torch.FloatTensor(word2vec_model.vectors) # shape 400kx100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb92cc7",
   "metadata": {},
   "source": [
    "## Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "633d207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_model = VisualModel()\n",
    "text_model = TextModel(pretrained_embeddings, hidden_dim=64)\n",
    "\n",
    "model = MultimodalDQN(visual_model, text_model, action_space=4)\n",
    "target_model = MultimodalDQN(visual_model, text_model, action_space=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4b42c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "replay_buffer = ReplayBuffer(capacity=1000)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc145f",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c3cf8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [torch.randn(3, 64, 64) for _ in range(6)]\n",
    "instructions = [\"find light switch\", \"locate power switch\", \"seek light control\", \n",
    "                \"discover light control\", \"identify switch light\", \"pinpoint light switch\"]\n",
    "\n",
    "actions = [0, 1, 1, 2, 0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f647e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch: 1\n",
      "--- Epoch: 2\n",
      "--- Epoch: 3\n",
      "--- Epoch: 4\n",
      "--- Epoch: 5\n",
      "--- Epoch: 6\n",
      "--- Epoch: 7\n",
      "--- Epoch: 8\n",
      "--- Epoch: 9\n",
      "--- Epoch: 10\n",
      "--- Epoch: 11\n",
      "--- Epoch: 12\n",
      "--- Epoch: 13\n",
      "--- Epoch: 14\n",
      "--- Epoch: 15\n",
      "--- Epoch: 16\n",
      "--- Epoch: 17\n",
      "--- Epoch: 18\n",
      "--- Epoch: 19\n",
      "--- Epoch: 20\n",
      "--- Epoch: 21\n",
      "--- Epoch: 22\n",
      "--- Epoch: 23\n",
      "--- Epoch: 24\n",
      "--- Epoch: 25\n",
      "--- Epoch: 26\n",
      "--- Epoch: 27\n",
      "--- Epoch: 28\n",
      "--- Epoch: 29\n",
      "--- Epoch: 30\n",
      "--- Epoch: 31\n",
      "--- Epoch: 32\n",
      "--- Epoch: 33\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 34\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 35\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 36\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 37\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 38\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 39\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 40\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 41\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 42\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 43\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 44\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 45\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 46\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 47\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 48\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 49\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- Epoch: 50\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n",
      "--- visual_features: torch.Size([32, 512])\n",
      "--- text_features: torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    \n",
    "    print(\"--- Epoch:\", epoch+1)\n",
    "    visual_state = images[epoch % len(images)]\n",
    "    text_state = instructions[epoch % len(instructions)]\n",
    "    text_state = [word2vec_model.key_to_index[word] for word in text_state.split()]\n",
    "    text_state = torch.LongTensor(text_state)\n",
    "    action = actions[epoch % len(actions)]\n",
    "    next_state = images[(epoch + 1) % len(images)]\n",
    "    \n",
    "    replay_buffer.push((visual_state, text_state, action, 1.0, next_state))\n",
    "    \n",
    "    if len(replay_buffer.memory) > 32:\n",
    "        batch = replay_buffer.sample(32)\n",
    "        q_learning_update(model, target_model, optimizer, batch, gamma)\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        target_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea91c5",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62397523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted action: 0\n"
     ]
    }
   ],
   "source": [
    "visual_input = images[0].unsqueeze(0)\n",
    "text_input = instructions[0]\n",
    "text_input = [word2vec_model.key_to_index[word] for word in text_input.split()]\n",
    "text_input = torch.LongTensor(text_input).unsqueeze(0)\n",
    "    \n",
    "q_values = model(visual_input, text_input)\n",
    "action = q_values.argmax().item()\n",
    "\n",
    "print(f\"Predicted action: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05028803",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
