{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25898f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import ai2thor\n",
    "import ai2thor_colab\n",
    "from ai2thor_colab import plot_frames\n",
    "from ai2thor.controller import Controller\n",
    "\n",
    "from ai2thor.platform import CloudRendering\n",
    "controller = Controller(platform=CloudRendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ae6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import DQN\n",
    "from utils import to_torchdim, frame2tensor, plot_durations, encode_feedback_nav, encode_feedback_nav_for_pickup, encode_feedback_pickup, encode_feedback_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91153cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f0819",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c8f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "floor_index = random.randint(0, 30)\n",
    "floor_index = 20\n",
    "\n",
    "controller = Controller(\n",
    "    agentMode = \"default\", # arm\n",
    "    visibilityDistance = 0.50,\n",
    "    scene = f\"FloorPlan{floor_index}\",\n",
    "\n",
    "    # step sizes\n",
    "    snapToGrid = True,\n",
    "    gridSize = 0.25,\n",
    "    rotateStepDegrees = 90,\n",
    "\n",
    "    # image modalities\n",
    "    renderInstanceSegmentation = False,\n",
    "    renderDepthImage = False,\n",
    "    renderSemanticSegmentation = False,\n",
    "    renderNormalsImage = False,\n",
    "    \n",
    "    # camera properties\n",
    "    width = 600,\n",
    "    height = 420,\n",
    "    fieldOfView = 120,\n",
    "    \n",
    "    # set seed for reproducability\n",
    "    seed = 90,\n",
    ")\n",
    "\n",
    "plot_frames(controller.last_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [(\"place the bowl on the table\", \"Bowl_89852f2b\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b15e7",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b65a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44857fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e4) # 2e4\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "NUM_EPISODES = 14000 # 6000\n",
    "TARGET_UPDATE = 4 # 8\n",
    "\n",
    "TAU = 1e-4 # 5e-5\n",
    "LR = 5.0e-4 # the best for attention is so far 5.0e-4\n",
    "\n",
    "SCREEN_WIDTH = SCREEN_HEIGHT = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [\"MoveAhead\", \"RotateLeft\", \"RotateRight\", \"PickupObject\", \"DropHandObject\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac87dfb",
   "metadata": {},
   "source": [
    "## Load Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57457d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = 'weights/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'weights/glove.6B.100d.txt.word2vec'\n",
    "\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412afc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = torch.FloatTensor(word2vec_model.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding\n",
    "word2vec_model.key_to_index['<PAD>'] = len(word2vec_model.key_to_index)\n",
    "\n",
    "new_embedding_vector = np.zeros((1, 100))\n",
    "new_embedding_vector = torch.tensor(new_embedding_vector, dtype=torch.float)\n",
    "pretrained_embeddings = torch.cat((pretrained_embeddings, new_embedding_vector), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7cacc0",
   "metadata": {},
   "source": [
    "## Set Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce965a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayMemory object.\"\"\"\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"visual_state\", \"text_state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def add(self, visual_state, text_state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to buffer.\"\"\"\n",
    "        \n",
    "        self.memory.append(self.experience(visual_state, text_state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        visual_states = torch.from_numpy(np.vstack([exp.visual_state.cpu().numpy() for exp in experiences if exp is not None])).float()\n",
    "        visual_states = visual_states.to(device)\n",
    "        \n",
    "        text_states = torch.from_numpy(np.vstack([exp.text_state.cpu().numpy() for exp in experiences if exp is not None])).float()\n",
    "        text_states = text_states.to(device)\n",
    "        \n",
    "        actions = torch.from_numpy(np.vstack([exp.action.cpu().numpy() for exp in experiences if exp is not None])).long()\n",
    "        actions = actions.to(device)\n",
    "        \n",
    "        rewards = torch.from_numpy(np.vstack([exp.reward.cpu().numpy() for exp in experiences if exp is not None])).float()\n",
    "        rewards = rewards.to(device)\n",
    "        \n",
    "        next_states = torch.from_numpy(np.vstack([exp.next_state.cpu().numpy() for exp in experiences if exp is not None])).float()\n",
    "        next_states = next_states.to(device)\n",
    "        \n",
    "        dones = torch.from_numpy(np.vstack([exp.done for exp in experiences if exp is not None]).astype(np.uint8)).float()\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        return (visual_states, text_states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        \n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12125b0",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, visual_feature_dim=512, textual_feature_dim=768):\n",
    "        \n",
    "        super(CrossModalAttention, self).__init__()\n",
    "        \n",
    "        self.visual_attention = nn.Linear(visual_feature_dim, textual_feature_dim)\n",
    "        self.textual_attention = nn.Linear(textual_feature_dim, textual_feature_dim)\n",
    "\n",
    "    def forward(self, visual_features, text_features):\n",
    "        \n",
    "        # Compute attention scores\n",
    "        visual_attention_scores = self.visual_attention(visual_features)\n",
    "        textual_attention_scores = self.textual_attention(text_features)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(visual_attention_scores + textual_attention_scores, dim=1)\n",
    "\n",
    "        # Apply attention weights to textual features\n",
    "        attended_text_features = attention_weights * text_features\n",
    "\n",
    "        return attended_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ff9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        \n",
    "        super(VisualModel, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.cnn(x)\n",
    "    \n",
    "    \n",
    "class TextModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_embedding, hidden_dim, seed):\n",
    "        \n",
    "        super(TextModel, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=True)\n",
    "        self.rnn = nn.LSTM(pretrained_embedding.shape[1], hidden_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.long()\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = output.view(output.shape[0], -1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb52924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, visual_model, text_model, attention, action_size, seed):\n",
    "        \n",
    "        super(MultimodalDQN, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.visual_model = visual_model\n",
    "        self.text_model = text_model\n",
    "        self.attention = attention\n",
    "        \n",
    "        # Define three fully connected layers\n",
    "        self.fc1 = nn.Linear(1280, 256) # 256, 512, 1024\n",
    "        self.fc2 = nn.Linear(256, action_size)\n",
    "        \n",
    "    def forward(self, visual_input, text_input):\n",
    "        \n",
    "        visual_features = self.visual_model(visual_input.to(device))\n",
    "        text_features = self.text_model(text_input.to(device))\n",
    "        \n",
    "        # print(\"--- DEBUG\")\n",
    "        # print(\"visual_features:\", visual_features.shape)\n",
    "        # print(\"text_features:\", text_features.shape)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attended_text_features = self.attention(visual_features, text_features)\n",
    "        \n",
    "        # Concatenate visual and text features\n",
    "        combined_features = torch.cat((visual_features, attended_text_features), dim=1)\n",
    "        \n",
    "        # Apply fully connected layers\n",
    "        combined_features = F.relu(self.fc1(combined_features))\n",
    "        q_values = self.fc2(combined_features)\n",
    "        \n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76c67b",
   "metadata": {},
   "source": [
    "## Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    \"\"\"The agent interacting with and learning from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, screen_width, screen_height, action_size, seed):\n",
    "        \"\"\"Init Agent’s models.\"\"\"\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Multimodal DQN\n",
    "        self.visual_model = VisualModel(seed=seed)\n",
    "        self.text_model = TextModel(pretrained_embeddings, hidden_dim=64, seed=seed)\n",
    "        self.attention = CrossModalAttention()\n",
    "        \n",
    "        self.dqn_net = MultimodalDQN(self.visual_model, self.text_model, self.attention, action_size, seed).to(device)\n",
    "        self.target_net = MultimodalDQN(self.visual_model, self.text_model, self.attention, action_size, seed).to(device)\n",
    "        self.optimizer = optim.RMSprop(self.dqn_net.parameters(), lr=LR, alpha=0.95, eps=0.01)\n",
    "        \n",
    "        # Replay Buffer\n",
    "        self.buffer = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.time_step = 0\n",
    "    \n",
    "    def memorize(self, visual_state, text_state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay buffer.\"\"\"\n",
    "        \n",
    "        self.buffer.add(visual_state, text_state, action, reward, next_state, done)\n",
    "    \n",
    "        self.time_step = (self.time_step + 1) % TARGET_UPDATE\n",
    "        if self.time_step == 0:\n",
    "            # if enough samples are available in memory, get random subset and learn\n",
    "            if len(self.buffer) > BATCH_SIZE:\n",
    "                experiences = self.buffer.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "    \n",
    "    def visual_preprocess(self, visual_state, screen_width, screen_height):\n",
    "        \"\"\"Preprocess input frame before passing into agent.\"\"\"\n",
    "        \n",
    "        resized_screen = Image.fromarray(visual_state).resize((screen_width, screen_height))\n",
    "        visual_state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "\n",
    "        return visual_state\n",
    "    \n",
    "    def text_preprocess(self, instruction):\n",
    "        \"\"\"Preprocess instructions before passing into agent.\"\"\"\n",
    "        \n",
    "        text_state = instruction\n",
    "        \n",
    "        longest_sentence_length = 12\n",
    "        sentence_length = len(text_state)\n",
    "        \n",
    "        padding = longest_sentence_length - len(text_state.split())\n",
    "        text_state = text_state + ''.join([' <PAD>' for _ in range(padding)])\n",
    "        \n",
    "        text_state = [word2vec_model.key_to_index[word] for word in text_state.split()]\n",
    "        text_state = torch.tensor(text_state).long()\n",
    "        text_state = text_state.unsqueeze(0)\n",
    "        \n",
    "        return text_state\n",
    "    \n",
    "    def act(self, visual_state, text_state, epsilon=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            self.dqn_net.eval()\n",
    "            with torch.no_grad():\n",
    "                action = self.dqn_net(visual_state, text_state).max(1)[1].view(1, 1)\n",
    "            self.dqn_net.train()\n",
    "            \n",
    "        else:\n",
    "            action = torch.tensor([[random.randrange(self.action_size)]], dtype=torch.long, device=device)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\"\"\"\n",
    "    \n",
    "        visual_states, text_states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # get index of maximum value for next state\n",
    "        Qsa_next = self.dqn_net(next_states, text_states).detach()\n",
    "        _, action_max = Qsa_next.max(1)\n",
    "        \n",
    "        # get max predicted Q values (for next states) from target network\n",
    "        Q_target_next = self.target_net(next_states, text_states).detach().gather(1, action_max.unsqueeze(1))\n",
    "        \n",
    "        # compute Q target\n",
    "        Q_target = rewards + (gamma * Q_target_next * (1 - dones))\n",
    "        \n",
    "        # get expected Q values from dqn network\n",
    "        Q_expected = self.dqn_net(visual_states, text_states).gather(1, actions)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(Q_target, Q_expected)\n",
    "        \n",
    "        # minimize the loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # update target network\n",
    "        self.soft_update(self.target_net, self.dqn_net, TAU)\n",
    "    \n",
    "    def soft_update(self, target_net, dqn_net, tau):\n",
    "        \"\"\"Soft update target network parameters.\"\"\"\n",
    "        \n",
    "        for target_param, dqn_param in zip(target_net.parameters(), dqn_net.parameters()):\n",
    "            target_param.data.copy_(tau*dqn_param.data + (1.0-tau) * target_param.data)\n",
    "    \n",
    "    def randomize_agent(self, controller):\n",
    "\n",
    "        positions = controller.step(\n",
    "            action=\"GetReachablePositions\"\n",
    "        ).metadata[\"actionReturn\"]\n",
    "\n",
    "        position = random.choice(positions)\n",
    "        controller.step(\n",
    "            action=\"Teleport\",\n",
    "            position=position,\n",
    "            rotation=dict(x=0, y=270, z=0),\n",
    "            horizon=0,\n",
    "            standing=True\n",
    "        )\n",
    "        \n",
    "    def watch(self, controller, instruction, action_space, num_episodes=10):\n",
    "        \"\"\"Watch trained agent.\"\"\"\n",
    "        \n",
    "        best_score = -np.inf\n",
    "        \n",
    "        data = [(\"switch\", \"LightSwitch_887b121a\"), (\"tomato\", \"Tomato_e65a6e2e\"), (\"garbage\", \"GarbageCan_d6916cf5\")]\n",
    "        object_dict = dict(data)\n",
    "\n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "            # initialize the environment and state\n",
    "            controller.reset(random=True)\n",
    "            \n",
    "            self.randomize_agent(controller)\n",
    "\n",
    "            visual_state = agent.visual_preprocess(controller.last_event.frame, \n",
    "                                               screen_width=SCREEN_WIDTH, screen_height=SCREEN_HEIGHT)\n",
    "        \n",
    "            instruction, AGENT_TARGET = instruction, object_dict[instruction]\n",
    "            text_state = agent.text_preprocess(instruction)\n",
    "            \n",
    "            total_score = 0\n",
    "\n",
    "            self.dqn_net.eval()\n",
    "            \n",
    "            for time_step in range(1, 50):\n",
    "                \n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                # select an action using the trained dqn network\n",
    "                with torch.no_grad():\n",
    "                    action = self.dqn_net(visual_state, text_state).max(1)[1].view(1, 1)\n",
    "\n",
    "                print(f\"Time Step: {time_step}, Action: {action_space[action.item()]}\")\n",
    "                event = controller.step(action = action_space[action.item()])\n",
    "\n",
    "                time.sleep(1.25)\n",
    "                \n",
    "                _, reward, done, _ = encode_feedback(event, controller, target_name=AGENT_TARGET)\n",
    "\n",
    "                # observe a new state\n",
    "                if not done:\n",
    "                    screen = controller.last_event.frame\n",
    "                    resized_screen = Image.fromarray(screen).resize((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "\n",
    "                    next_state = frame2tensor(to_torchdim(resized_screen)).to(torch.float32).to(device)\n",
    "                else:\n",
    "                    time.sleep(1.25)\n",
    "                    next_state = None\n",
    "\n",
    "                visual_state = next_state\n",
    "                total_score += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if total_score > best_score: \n",
    "                best_score = total_score\n",
    "\n",
    "            print(f'\\rEpisode {i_episode}/{num_episodes}, Total Score: {total_score}, Best Score: {best_score}', end='') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(screen_width=SCREEN_WIDTH, screen_height=SCREEN_HEIGHT, action_size=len(action_space), seed=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {\"Total\": total_params, \"Trainable\": trainable_params}\n",
    "\n",
    "param_counts = count_parameters(agent.dqn_net)\n",
    "print(f\"Total Parameters: {param_counts['Total']}\")\n",
    "print(f\"Trainable Parameters: {param_counts['Trainable']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc70af",
   "metadata": {},
   "source": [
    "## Modify Taks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0325ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [(\"find bowl take it then place it on the table\", \"Bowl_89852f2b\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e0431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [\"MoveAhead\", \"RotateLeft\", \"RotateRight\", \"PickupObject\", \"DropHandObject\"]\n",
    "action_size = len(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b437681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.dqn_net.load_state_dict(torch.load(f'./agents/AI2THOR_MM_RL_ATT_TASK5b.pth'));\n",
    "# agent.target_net.load_state_dict(torch.load(f'./agents/AI2THOR_MM_RL_ATT_TASK5b.pth'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.action_size = action_size\n",
    "\n",
    "agent.dqn_net.fc2 = nn.Linear(256, action_size); agent.dqn_net.to(device);\n",
    "agent.target_net.fc2 = nn.Linear(256, action_size); agent.target_net.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5632f",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define linear decay\n",
    "def calculate_epsilon(episode):\n",
    "    slope = (EPS_END - EPS_START) / NUM_EPISODES\n",
    "    epsilon = EPS_START + slope * episode\n",
    "\n",
    "    return max(epsilon, EPS_END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_agent(controller):\n",
    "    \n",
    "    positions = controller.step(\n",
    "        action=\"GetReachablePositions\"\n",
    "    ).metadata[\"actionReturn\"]\n",
    "\n",
    "    position = random.choice(positions)\n",
    "    controller.step(\n",
    "        action=\"Teleport\",\n",
    "        position=position,\n",
    "        rotation=dict(x=0, y=270, z=0),\n",
    "        horizon=0,\n",
    "        standing=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(num_episodes, max_time):\n",
    "    \n",
    "    epsilon = EPS_START\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        # initialize the environment and state\n",
    "        controller.reset(random=True)\n",
    "        \n",
    "        randomize_agent(controller)\n",
    "        is_first_visit = True\n",
    "        \n",
    "        visual_state = agent.visual_preprocess(controller.last_event.frame, \n",
    "                                               screen_width=SCREEN_WIDTH, screen_height=SCREEN_HEIGHT)\n",
    "        \n",
    "        inst_tupple = instructions[i_episode % len(instructions)]\n",
    "        instruction, AGENT_TARGET = inst_tupple\n",
    "        text_state = agent.text_preprocess(instruction)\n",
    "        \n",
    "        total_score = 0\n",
    "        \n",
    "        for time_step in range(1, max_time+1):\n",
    "            \n",
    "            # select and perform an action using dqn network\n",
    "            action = agent.act(visual_state, text_state, epsilon)\n",
    "            scalar_action = action.item()\n",
    "            \n",
    "            if (scalar_action == 4):\n",
    "                event = controller.step(action=action_space[scalar_action], forceAction=False)\n",
    "                _, reward, done, _ = encode_feedback_drop(event, controller, target_name=AGENT_TARGET)\n",
    "            elif (scalar_action == 3):\n",
    "                event = controller.step(action = action_space[scalar_action], \n",
    "                                        objectId = \"Bowl|+01.59|+00.90|-01.26\", \n",
    "                                        forceAction=False, manualInteract=False)\n",
    "                _, reward, done, _ = encode_feedback_pickup(event, controller, target_name=AGENT_TARGET)\n",
    "            else:\n",
    "                event = controller.step(action = action_space[scalar_action])\n",
    "                    \n",
    "                if is_first_visit:\n",
    "                    _, reward, done, _ = encode_feedback_nav(event, controller, target_name=AGENT_TARGET)\n",
    "                    \n",
    "                    if reward == +10.0: \n",
    "                        is_first_visit = False\n",
    "                else: \n",
    "                    _, reward, done, _ = encode_feedback_nav_for_pickup(event, controller)\n",
    "            \n",
    "            # time.sleep(1)\n",
    "            \n",
    "            total_score += reward\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            \n",
    "            next_state = agent.visual_preprocess(controller.last_event.frame, \n",
    "                                                 screen_width=SCREEN_WIDTH, screen_height=SCREEN_HEIGHT)\n",
    "            \n",
    "            agent.memorize(visual_state, text_state, action, reward, next_state, done)\n",
    "            \n",
    "            # move to the next state\n",
    "            visual_state = next_state\n",
    "             \n",
    "            if done or time_step == max_time:\n",
    "                plot_durations(total_score, i_episode, num_episodes, instruction, total_score, time_step)\n",
    "                break\n",
    "            \n",
    "        epsilon = calculate_epsilon(i_episode)\n",
    "        \n",
    "    if not os.path.exists('./agents/'): os.makedirs('./agents/')\n",
    "    torch.save(agent.dqn_net.state_dict(), f'./agents/AI2THOR_MM_RL_ATT_TASK6b.pth')\n",
    "    \n",
    "    # print('Training completed.')\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1f33a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Training the network...')\n",
    "train_network(num_episodes=NUM_EPISODES, max_time=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eba9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚩 Chatgpt\n",
    "# https://chat.openai.com/c/3faffdb2-8abe-4453-b501-357d27fa2c7a\n",
    "\n",
    "# 1. define action ✅\n",
    "# 2. modify neural network ✅\n",
    "# 3. modify training process (agent's position, reward assignment) ✅\n",
    "# 4. modify reward design ✅\n",
    "# 5. check everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfded35",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
