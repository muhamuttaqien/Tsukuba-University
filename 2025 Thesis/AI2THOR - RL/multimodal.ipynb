{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a223a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple environment (in this case, a list of images and text instructions).\n",
    "# In a real scenario, you would collect data from your simulator.\n",
    "images = [torch.randn(3, 64, 64) for _ in range(10)]\n",
    "text_instructions = [\"find light switch\" for _ in range(10)]\n",
    "actions = [0, 1, 1, 0, 0, 1, 0, 1, 0, 1]  # Example actions (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4483e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN model to extract visual features from images.\n",
    "class VisualModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VisualModel, self).__init__()\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()  # Remove the final classification layer.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "# Define an NLP model to process text instructions.\n",
    "class TextModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        return output[-1]  # Use the final hidden state as text representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ebfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the multimodal DQN model.\n",
    "class MultimodalDQN(nn.Module):\n",
    "    def __init__(self, visual_model, text_model, action_space):\n",
    "        super(MultimodalDQN, self).__init__()\n",
    "        self.visual_model = visual_model\n",
    "        self.text_model = text_model\n",
    "        self.fc = nn.Linear(512 + 256, action_space)  # Combine visual and text representations.\n",
    "\n",
    "    def forward(self, visual_input, text_input):\n",
    "        visual_features = self.visual_model(visual_input)\n",
    "        text_features = self.text_model(text_input)\n",
    "        combined_features = torch.cat((visual_features, text_features), dim=1)\n",
    "        q_values = self.fc(combined_features)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e600170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple replay buffer for experience replay.\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory.pop(0)\n",
    "            self.memory.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-learning algorithm for training the agent.\n",
    "def q_learning_update(model, target_model, optimizer, batch, gamma):\n",
    "    state, action, reward, next_state = zip(*batch)\n",
    "\n",
    "    state = torch.stack(state)\n",
    "    action = torch.tensor(action)\n",
    "    reward = torch.tensor(reward)\n",
    "    next_state = torch.stack(next_state)\n",
    "\n",
    "    q_values = model(state)\n",
    "    next_q_values = target_model(next_state).max(1).values.detach()\n",
    "    expected_q_values = reward + gamma * next_q_values\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss_value = loss(q_values.gather(1, action.unsqueeze(1)), expected_q_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_value.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f77414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models, replay buffer, and optimizer.\n",
    "visual_model = VisualModel()\n",
    "text_model = TextModel(vocab_size=10000, embedding_dim=64, hidden_dim=64)  # Adjust the values accordingly.\n",
    "\n",
    "model = MultimodalDQN(visual_model, text_model, action_space=2)\n",
    "target_model = MultimodalDQN(visual_model, text_model, action_space=2)  # Use a separate target network for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7781c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "replay_buffer = ReplayBuffer(capacity=1000)\n",
    "gamma = 0.9  # Discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937a9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop.\n",
    "for epoch in range(1000):\n",
    "    state = images[epoch % len(images)]\n",
    "    text_input = text_instructions[epoch % len(text_instructions)]\n",
    "    action = actions[epoch % len(actions)]\n",
    "    next_state = images[(epoch + 1) % len(images)]\n",
    "\n",
    "    replay_buffer.push((state, action, 1.0, next_state))  # Assume a reward of 1.0 for simplicity.\n",
    "\n",
    "    if len(replay_buffer.memory) > 32:  # Start training once enough samples are available.\n",
    "        batch = replay_buffer.sample(32)\n",
    "        q_learning_update(model, target_model, optimizer, batch, gamma)\n",
    "\n",
    "    if epoch % 100 == 0:  # Update the target network every 100 epochs.\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "# Use the trained model for inference.\n",
    "visual_input = images[0]\n",
    "text_input = text_instructions[0]\n",
    "q_values = model(visual_input, text_input)\n",
    "action_to_take = q_values.argmax().item()\n",
    "\n",
    "print(f\"Predicted action: {action_to_take}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # text_vector = [vocab[word] for word in text_vector.split()]\n",
    "    # text_vector = torch.LongTensor(text_vector)\n",
    "    # text_state = text_model(text_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05028803",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
